# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B65O7Ma9wnpJ5GkegnAQ0LfkwYU7QcDx
"""

import pyswarms as ps

X=np.load("X_train.npy") # shape (30926,700)
y=np.load("y_train.npy") # shape (30926,932)

n_inputs = 4096
n_hidden = 1024
n_classes = 932

def binary_cross_entropy(y, y_hat):
    left = y * np.log(y_hat + 1e-7)
    right = (1 - y) * np.log((1 - y_hat) + 1e-7)
    return -np.mean(left + right)

def sigmoid(logit):
    return 1 / (1 + np.exp(-logit))


def forward_prop(params):
    """Forward propagation as objective function

    This computes for the forward propagation of the neural network, as
    well as the loss. It receives a set of parameters that must be
    rolled-back into the corresponding weights and biases.

    Inputs
    ------
    params: np.ndarray
        The dimensions should include an unrolled version of the
        weights and biases.

    Returns
    -------
    float
        The computed negative log-likelihood loss given the parameters
    """
    # Neural network architecture
    n_inputs = 4096
    n_hidden = 1024
    n_classes = 932
    
    inputs=n_inputs*n_hidden

    # Roll-back the weights and biases
    W1 = params[0:inputs].reshape((n_inputs,n_hidden))
    b1 = params[inputs:inputs+n_hidden].reshape((n_hidden,))
    W2 = params[inputs+n_hidden:inputs+n_hidden+n_hidden*n_classes].reshape((n_hidden,n_classes))
    b2 = params[inputs+n_hidden+n_hidden*n_classes:inputs+n_hidden+n_hidden*n_classes+n_classes].reshape((n_classes,))

    # Perform forward propagation
    z1 = X.dot(W1) + b1  # Pre-activation in Layer 1
    a1 = np.tanh(z1)     # Activation in Layer 1
    z2 = a1.dot(W2) + b2 # Pre-activation in Layer 2
    logits = z2          # Logits for Layer 2
    probs=sigmoid(logits) # sigmoid on last layer 

    return binary_cross_entropy(y, probs) # binary_cross_entropy loss

def f(x):
    """Higher-level method to do forward_prop in the
    whole swarm.

    Inputs
    ------
    x: numpy.ndarray of shape (n_particles, dimensions)
        The swarm that will perform the search

    Returns
    -------
    numpy.ndarray of shape (n_particles, )
        The computed loss for each particle
    """
    n_particles = x.shape[0]
    j = [forward_prop(x[i]) for i in range(n_particles)]
    
    return np.array(j)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Initialize swarm
# options = {'c1': 0.5, 'c2': 0.3, 'w':0.9} 
# # Call instance of PSO
# dimensions = (n_inputs* n_hidden ) + (n_hidden * n_classes) + n_hidden + n_classes
# 
# optimizer = ps.single.GlobalBestPSO(n_particles=100, dimensions=dimensions, options=options) # set the n_particles 
# 
# # Perform optimization 
# cost, pos = optimizer.optimize(f, iters=1000) # set iterers

def prediction(X,pos):
    """Forward propagation as objective function

    This computes for the forward propagation of the neural network, as
    well as the loss. It receives a set of parameters that must be
    rolled-back into the corresponding weights and biases.

    Inputs
    ------
    params: np.ndarray
        The dimensions should include an unrolled version of the
        weights and biases.

    Returns
    -------
    float
        The computed negative log-likelihood loss given the parameters
    """
    n_inputs = 4096
    n_hidden = 1024
    n_classes = 932
    
    inputs=n_inputs*n_hidden

    # Roll-back the weights and biases
    W1 = params[0:inputs].reshape((n_inputs,n_hidden))
    b1 = params[inputs:inputs+n_hidden].reshape((n_hidden,))
    W2 = params[inputs+n_hidden:inputs+n_hidden+n_hidden*n_classes].reshape((n_hidden,n_classes))
    b2 = params[inputs+n_hidden+n_hidden*n_classes:inputs+n_hidden+n_hidden*n_classes+n_classes].reshape((n_classes,))


    # Perform forward propagation
    z1 = X.dot(W1) + b1  # Pre-activation in Layer 1
    a1 = np.tanh(z1)     # Activation in Layer 1
    z2 = a1.dot(W2) + b2 # Pre-activation in Layer 2
    logits = z2          # Logits for Layer 2
    y_pred=[]
    for ix in logits:
      c=[sigmoid(ixx) for ixx in ix]
      y_pred.append(c)
    return np.array(y_pred)

y_pred=prediction(X, pos)